---
layout: post
title:  "Tutorial: Image Classification & Transfer Learning Tutorial"
categories: blog assignment
permalink: posts/HW-3
author: Shannan Liu
---

## Introduction
In this blog post, we're going to explore image classification with convolutional neural networks. More specifically, we're going to build successively complex models to perform image classification so that we can explore tools such as data augmentation, image preprocessing, and transfer learning.

## 1. Load Packages and Obtain Data

First, let's obtain the relevant imports and data

```python
import os
import tensorflow as tf
from tensorflow.keras import utils, layers, models

import numpy as np

import matplotlib as mpl
import matplotlib.pyplot as plt
```

Get sample dataset from Tensorflow team. The data is split into a training, validation, and testing set.

```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

### Working with Datasets
Before we begin modelling, we should gain some familiarity with using Tensorflow Datasets.

Here, we'll demonstrate a two row visualisation of cats & dogs with the training dataset. The first row will be composed of random cat pictures, and the second row will be composed of random dog pictures.

```python
# first, we'll extract class names from dataset
class_names = train_dataset.class_names
class_names

# now let's create our visualisation function
def plot_animals():
  """
  create a two-row visusalisation of cats & dogs in the training dataset
  """
  plt.figure(figsize=(10, 10)) # set plot figsize

  # get images and labels from training set
  for images, labels in train_dataset.take(1):
    # get list of cat images
    cat_images = images[np.array(np.array(labels) == 0)]

    # get list of dog images
    dog_images = images[np.array(np.array(labels) == 1)]

    # plot 2 rows of 3 images of cats and dogs - total 6 images
    for i in range(6):

      # set up subplots
      ax = plt.subplot(2, 3, i + 1)
      if i < 3: # plot cats in 1st row
        plt.imshow(cat_images[i].numpy().astype("uint8"))
        plt.title('cats')
        plt.axis("off")
      else: # plot dogs in 2nd row
        plt.imshow(dog_images[i].numpy().astype("uint8"))
        plt.title('dogs')
        plt.axis("off")
```

Using the function
```python
plot_animals() # run function
```
![_config.yml]({{ site.baseurl }}/images/img-class-1-hw3.png)


### Check Label Frequencies
Next, we should check our label frequencies. This is important because a baseline model will always guess the label with the highest frequency. Thus, if the number of cat and dog labels in our dataset are not balanced, then our model's baseline performance will not be 50%, meaning we have to judge our model by another benchmark (i.e. being better than 60% is considered a good model if our model has 60% cat images and 40% dog images for instance).

However, before we do that, let's add some code that will enable us to read our data more efficiently.

```python
# related to reading data more rapidly
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

Now we check the frequency of our data's labels. Below, I show two methods for doing so. The first method uses an iterator, and the second method uses a for loop.

```python
# using iterator to compute number of labels corresponding to cat (0) and dog (1)
labels_iterator = train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
print(f"Number of cats = {np.sum(np.array(list(labels_iterator))==0)}")
labels_iterator = train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
print(f"Number of dogs = {np.sum(np.array(list(labels_iterator))==1)}")

Number of cats = 1000
Number of dogs = 1000
```
```python
# alternative method
# using for loop to compute number of labels corresponding to cat (0) and dog (1)
dog_count = 0
cat_count = 0
for images, labels in train_dataset.take(len(train_dataset)):
  dog_count += sum(np.array(labels))
  cat_count += sum(np.array(labels) == 0)

print(f"Number of cats = {cat_count}")
print(f"Number of dogs = {dog_count}")

Number of cats = 1000
Number of dogs = 1000
```
In this case, our baseline model would have an accuracy of 50% because there is an even split in the number of dogs and cats in our dataset. This is perfect, and weâ€™ll treat this as the benchmark for improvement. Our models should do much better than baseline to be considered good data science achievements.


## 2. First Model
Now we'll move onto builing our first model, which will be comprised of (1) convolutional layers that help us extract features from our images and (2) dense layers that will perform the classification task for us. Our output comprises of 2 neurons because we want to make predictions on 2 classes.

```python
model1 = tf.keras.Sequential([
  # feature extraction
  layers.Conv2D(64,(3,3),activation = 'relu', input_shape = (160,160,3)),
  layers.BatchNormalization(),
  layers.MaxPooling2D((2,2)),
  layers.Conv2D(32,(3,3),activation = 'relu'),
  layers.BatchNormalization(),
  layers.MaxPooling2D((2,2)),
  layers.Conv2D(32,(3,3,),activation = 'relu'),
  layers.BatchNormalization(),
  layers.MaxPooling2D((2,2)),
  layers.Conv2D(32,(3,3,),activation = 'relu'),
  layers.BatchNormalization(),
  layers.MaxPooling2D((2,2)),
  layers.Flatten(),

  # classification
  layers.Dense(128,activation = 'relu'),
  layers.Dropout(0.2),
  layers.Dense(256,activation = 'relu'),
  layers.Dropout(0.2),
  layers.Dense(256,activation = 'relu'),
  layers.Dropout(0.2),
  layers.Dense(128,activation = 'relu'),
  layers.Dense(2) # output layer
])
```
The model summary below shows us the number of parameters in our model.
```python
model1.summary()
```
```
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d (Conv2D)             (None, 158, 158, 64)      1792

 batch_normalization (BatchN  (None, 158, 158, 64)     256
 ormalization)

 max_pooling2d (MaxPooling2D  (None, 79, 79, 64)       0
 )

 conv2d_1 (Conv2D)           (None, 77, 77, 32)        18464

 batch_normalization_1 (Batc  (None, 77, 77, 32)       128
 hNormalization)

 max_pooling2d_1 (MaxPooling  (None, 38, 38, 32)       0
 2D)

 conv2d_2 (Conv2D)           (None, 36, 36, 32)        9248

 batch_normalization_2 (Batc  (None, 36, 36, 32)       128
 hNormalization)

 max_pooling2d_2 (MaxPooling  (None, 18, 18, 32)       0
 2D)

 conv2d_3 (Conv2D)           (None, 16, 16, 32)        9248

 batch_normalization_3 (Batc  (None, 16, 16, 32)       128
 hNormalization)

 max_pooling2d_3 (MaxPooling  (None, 8, 8, 32)         0
 2D)

 flatten (Flatten)           (None, 2048)              0

 dense (Dense)               (None, 128)               262272

 dropout (Dropout)           (None, 128)               0

 dense_1 (Dense)             (None, 256)               33024

 dropout_1 (Dropout)         (None, 256)               0

 dense_2 (Dense)             (None, 256)               65792

 dropout_2 (Dropout)         (None, 256)               0

 dense_3 (Dense)             (None, 128)               32896

 dense_4 (Dense)             (None, 2)                 258

=================================================================
Total params: 433,634
Trainable params: 433,314
Non-trainable params: 320
_________________________________________________________________
```

Now, we move onto training the model.
```python
# compile our model with relevant optimizer, loss function, and metrics
model1.compile(optimizer = 'adam',
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), # from_logits = True means to apply softmax layer at end
              metrics = ['accuracy'])
              # fit and train the model
history1 = model1.fit(train_dataset,
                      epochs = 20,
                      validation_data = validation_dataset)
```
```
Epoch 1/20
63/63 [==============================] - 19s 121ms/step - loss: 0.7377 - accuracy: 0.5530 - val_loss: 1.0626 - val_accuracy: 0.5037
Epoch 2/20
63/63 [==============================] - 7s 107ms/step - loss: 0.6513 - accuracy: 0.6145 - val_loss: 0.6364 - val_accuracy: 0.6312
Epoch 3/20
63/63 [==============================] - 7s 105ms/step - loss: 0.5903 - accuracy: 0.6820 - val_loss: 0.7609 - val_accuracy: 0.5916
Epoch 4/20
63/63 [==============================] - 7s 106ms/step - loss: 0.5315 - accuracy: 0.7265 - val_loss: 0.6424 - val_accuracy: 0.6572
Epoch 5/20
63/63 [==============================] - 7s 104ms/step - loss: 0.4969 - accuracy: 0.7605 - val_loss: 0.8033 - val_accuracy: 0.6436
Epoch 6/20
63/63 [==============================] - 7s 113ms/step - loss: 0.4397 - accuracy: 0.7945 - val_loss: 1.0268 - val_accuracy: 0.6151
Epoch 7/20
63/63 [==============================] - 7s 105ms/step - loss: 0.3574 - accuracy: 0.8370 - val_loss: 0.8123 - val_accuracy: 0.6720
Epoch 8/20
63/63 [==============================] - 7s 107ms/step - loss: 0.3142 - accuracy: 0.8650 - val_loss: 0.8900 - val_accuracy: 0.6411
Epoch 9/20
63/63 [==============================] - 7s 110ms/step - loss: 0.2828 - accuracy: 0.8865 - val_loss: 0.7902 - val_accuracy: 0.6906
Epoch 10/20
63/63 [==============================] - 7s 106ms/step - loss: 0.2060 - accuracy: 0.9260 - val_loss: 0.7604 - val_accuracy: 0.7166
Epoch 11/20
63/63 [==============================] - 7s 105ms/step - loss: 0.1616 - accuracy: 0.9360 - val_loss: 0.7744 - val_accuracy: 0.7079
Epoch 12/20
63/63 [==============================] - 7s 105ms/step - loss: 0.1665 - accuracy: 0.9355 - val_loss: 1.1548 - val_accuracy: 0.6856
Epoch 13/20
63/63 [==============================] - 7s 107ms/step - loss: 0.1097 - accuracy: 0.9610 - val_loss: 0.9686 - val_accuracy: 0.7203
Epoch 14/20
63/63 [==============================] - 8s 123ms/step - loss: 0.1136 - accuracy: 0.9630 - val_loss: 1.0452 - val_accuracy: 0.7005
Epoch 15/20
63/63 [==============================] - 10s 151ms/step - loss: 0.0897 - accuracy: 0.9700 - val_loss: 0.9746 - val_accuracy: 0.6980
Epoch 16/20
63/63 [==============================] - 8s 123ms/step - loss: 0.0904 - accuracy: 0.9695 - val_loss: 0.9831 - val_accuracy: 0.6696
Epoch 17/20
63/63 [==============================] - 9s 137ms/step - loss: 0.0651 - accuracy: 0.9735 - val_loss: 0.9886 - val_accuracy: 0.7252
Epoch 18/20
63/63 [==============================] - 8s 124ms/step - loss: 0.0572 - accuracy: 0.9780 - val_loss: 1.2313 - val_accuracy: 0.6918
Epoch 19/20
63/63 [==============================] - 7s 109ms/step - loss: 0.0422 - accuracy: 0.9855 - val_loss: 1.3125 - val_accuracy: 0.6980
Epoch 20/20
63/63 [==============================] - 8s 123ms/step - loss: 0.0681 - accuracy: 0.9825 - val_loss: 1.1001 - val_accuracy: 0.7030
```

Plot of our model's performance
```python
def plot_history(history):
  # plot performance of training and validation sets
  plt.plot(history.history["accuracy"], label = "training")
  plt.plot(history.history["val_accuracy"], label = "validation")
  plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
  plt.title("Training vs Validation Accuracy")
  plt.legend()
  plt.show()

plot_history(history1)
```
![_config.yml]({{ site.baseurl }}/images/mod1-perf-hw3.png)

In the plot above, we can see that the validation accuracy of the model stabilises **between 60% - 70%** while training. This is roughly 10% - 20% better than a baseline model, which would have an accuracy of 50%. Overfitting can be observed over the entire training process, but it becomes a particularly big issue after the 5th epoch, when training accuracy is about 80% - 90% while the validation accuracy stabilises in the range of 60% - 70%.

## 3. Model with Data Augmentation
We're going to improve the model by adding data augmentation layers to it. Data augmentation refers to the practice of including modified copies of the same image in the training set. For example, a picture of a cat is still a picture of a cat even if we flip it upside down or rotate it 90 degrees. We can include such transformed versions of the image in our training process in order to help our model learn so-called invariant features of our input images.

We're going to augment our data using the `tf.keras.layers.RandomFlip()` and `tf.keras.layers.RandomRotation()` layers. The first layer will be used to flip images horizontally and the second one will rotate our images by a specific factor.

I'll first demonstrate what `tf.keras.layers.RandomFlip()` does, then I'll show what  `tf.keras.layers.RandomRotation()` does. Finally, I'll demonstrate how to use them in combination.

### Example of Data Augmentation
Horizontal flip using `tf.keras.layers.RandomFlip()`:
```python
# randomly flip an image horizontally
rand_flip =  tf.keras.Sequential([
  layers.RandomFlip('horizontal')
])

# randomly rotate an image by a factor of 0.2
rand_rot =  tf.keras.Sequential([
  layers.RandomRotation(0.2)
])

# visualisation for randomly rotating an image in our training set
for image, _ in train_dataset.take(1):
  # get image
  im = tf.cast(tf.expand_dims(image[0], 0), tf.float32)

  # configure plot settings
  plt.figure(figsize=(10, 10))

  # plot non-augmented image
  plt.subplots(1,1)
  plt.imshow(image[0].numpy().astype("uint8"))
  plt.axis("off") # remove image axes

  # plot rotated image
  plt.subplots(1,1)
  augmented_image = rand_rot(im)
  plt.imshow(augmented_image[0]/255)
  plt.axis("off")
```
![_config.yml]({{ site.baseurl }}/images/norm-rot-hw3.png)
![_config.yml]({{ site.baseurl }}/images/rot-hw3.png)

Random rotation using `tf.keras.layers.RandomRotation()`:
```python
# visualisation for horizontally flipping an image
# in our training set
for image, _ in train_dataset.take(1):
  # get image
  im = tf.cast(tf.expand_dims(image[0], 0), tf.float32)

  # plot settings
  plt.figure(figsize=(10, 10))

  # plot non-augmented image
  plt.subplots(1,1)
  plt.imshow(image[0].numpy().astype("uint8"))
  plt.axis("off")

  # plot flipped image
  plt.subplots(1,1)
  augmented_image = rand_flip(im)
  plt.imshow(augmented_image[0]/255)
  plt.axis("off")
```
![_config.yml]({{ site.baseurl }}/images/norm-flip-hw3.png)
![_config.yml]({{ site.baseurl }}/images/flip-hw3.png)

Using both `tf.keras.layers.RandomFlip()` & `tf.keras.layers.RandomRotation()` for data augmentation:
```python
for image, _ in train_dataset.take(1):
  # plot settings
  plt.figure(figsize=(10, 10))

  # get image
  first_image = image[0]
  first_image = tf.cast(tf.expand_dims(first_image, 0), tf.float32)

  # augment the image 9 times and plot the augmented images
  for i in range(9):
    augmented_image = data_augmentation(first_image) # apply augmentation

    # plot
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(augmented_image[0]/255)
    plt.axis('off')
```
![_config.yml]({{ site.baseurl }}/images/data-aug-hw3.png)

### Model 2 Training and Performance
Now let's build a model that incorporates data augmentation. We'll call this model `model2`.

```python
# building model 2
model2 = tf.keras.Sequential([
  # adding data augmentation
  layers.RandomFlip('horizontal'),
  layers.RandomRotation(0.2),

  # feature extraction
  layers.Conv2D(64,(3,3),activation = 'relu', input_shape = (160,160,3)),
  layers.BatchNormalization(),
  layers.MaxPooling2D((2,2)),
  layers.Conv2D(32,(3,3),activation = 'relu'),
  layers.BatchNormalization(),
  layers.MaxPooling2D((2,2)),
  layers.Conv2D(32,(3,3,),activation = 'relu'),
  layers.BatchNormalization(),
  layers.MaxPooling2D((2,2)),
  layers.Conv2D(32,(3,3,),activation = 'relu'),
  layers.BatchNormalization(),
  layers.MaxPooling2D((2,2)),
  layers.Flatten(),

  # classification
  layers.Dense(128,activation = 'relu'),
  layers.Dropout(0.2),
  layers.Dense(256,activation = 'relu'),
  layers.Dropout(0.2),
  layers.Dense(256,activation = 'relu'),
  layers.Dropout(0.2),
  layers.Dense(128,activation = 'relu'),
  layers.Dense(2) # output layer
])

# compile model
model2.compile(optimizer = 'adam',
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), # from_logits = True means to apply softmax layer at end
              metrics = ['accuracy'])
```

Training the model
```python
# train model2
history2 = model2.fit(train_dataset,epochs = 20,
           validation_data = validation_dataset)
```
```
Epoch 1/20
63/63 [==============================] - 10s 113ms/step - loss: 0.7535 - accuracy: 0.5375 - val_loss: 0.8409 - val_accuracy: 0.4913
Epoch 2/20
63/63 [==============================] - 7s 109ms/step - loss: 0.6701 - accuracy: 0.6075 - val_loss: 0.6465 - val_accuracy: 0.6101
Epoch 3/20
63/63 [==============================] - 8s 129ms/step - loss: 0.6421 - accuracy: 0.6350 - val_loss: 0.8282 - val_accuracy: 0.4975
Epoch 4/20
63/63 [==============================] - 8s 124ms/step - loss: 0.6198 - accuracy: 0.6590 - val_loss: 0.6163 - val_accuracy: 0.6621
Epoch 5/20
63/63 [==============================] - 9s 129ms/step - loss: 0.5925 - accuracy: 0.6880 - val_loss: 0.6729 - val_accuracy: 0.6498
Epoch 6/20
63/63 [==============================] - 9s 134ms/step - loss: 0.5734 - accuracy: 0.7005 - val_loss: 0.9416 - val_accuracy: 0.5334
Epoch 7/20
63/63 [==============================] - 9s 141ms/step - loss: 0.5632 - accuracy: 0.7150 - val_loss: 0.9271 - val_accuracy: 0.5705
Epoch 8/20
63/63 [==============================] - 9s 132ms/step - loss: 0.5524 - accuracy: 0.7205 - val_loss: 0.5697 - val_accuracy: 0.6968
Epoch 9/20
63/63 [==============================] - 8s 119ms/step - loss: 0.5456 - accuracy: 0.7195 - val_loss: 0.5775 - val_accuracy: 0.6906
Epoch 10/20
63/63 [==============================] - 8s 126ms/step - loss: 0.5367 - accuracy: 0.7325 - val_loss: 0.6398 - val_accuracy: 0.6324
Epoch 11/20
63/63 [==============================] - 9s 129ms/step - loss: 0.5014 - accuracy: 0.7580 - val_loss: 0.8122 - val_accuracy: 0.5842
Epoch 12/20
63/63 [==============================] - 8s 109ms/step - loss: 0.5192 - accuracy: 0.7470 - val_loss: 1.0362 - val_accuracy: 0.5668
Epoch 13/20
63/63 [==============================] - 7s 108ms/step - loss: 0.5083 - accuracy: 0.7615 - val_loss: 0.5558 - val_accuracy: 0.7042
Epoch 14/20
63/63 [==============================] - 7s 108ms/step - loss: 0.4963 - accuracy: 0.7530 - val_loss: 0.5610 - val_accuracy: 0.7030
Epoch 15/20
63/63 [==============================] - 8s 123ms/step - loss: 0.4818 - accuracy: 0.7675 - val_loss: 0.5934 - val_accuracy: 0.6795
Epoch 16/20
63/63 [==============================] - 8s 119ms/step - loss: 0.4934 - accuracy: 0.7695 - val_loss: 0.5257 - val_accuracy: 0.7376
Epoch 17/20
63/63 [==============================] - 8s 126ms/step - loss: 0.4815 - accuracy: 0.7680 - val_loss: 0.6813 - val_accuracy: 0.6856
Epoch 18/20
63/63 [==============================] - 9s 133ms/step - loss: 0.4668 - accuracy: 0.7800 - val_loss: 0.6432 - val_accuracy: 0.6262
Epoch 19/20
63/63 [==============================] - 9s 130ms/step - loss: 0.4674 - accuracy: 0.7945 - val_loss: 0.5231 - val_accuracy: 0.7401
Epoch 20/20
63/63 [==============================] - 8s 117ms/step - loss: 0.4487 - accuracy: 0.7910 - val_loss: 0.5421 - val_accuracy: 0.7500
```
```python
# plot of model2 performance in training and validation sets
plot_history(model2)
```
![_config.yml]({{ site.baseurl }}/images/mod2-perf-hw3.png)

The validation accuracy of the model stabilised **between 60% - 70%** while training. This is similar to the performance of model1. However, we can see that there is significantly less overfitting in model2 than there is in model1. That is, the model's training performance more closely matches the validation performance.

## 4. Data Preprocessing
Sometimes, it can be helpful to make simple transformations to the input data. For example, in this case, the original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0 and 1, or possibly between -1 and 1. These are mathematically identical situations, since we can always just scale the weights. But if we handle the scaling prior to the training process, we can spend more of our training energy handling actual signal in the data and less energy having the weights adjust to the data scale.

Thus, we will now add a preprocessing layer to our model to see if we can improve its performance.

```python
# creating the preprocessing layer
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```
```python
# building model 3
# input shape of our images
i = tf.keras.Input(shape=(160, 160, 3))
# preprocessing
x = preprocessor(i)
# data augmentation
x = data_augmentation(x)

# Model2
# feature extraction with convolutional layers
x = layers.Conv2D(64,(3,3),activation = 'relu')(x)
x = layers.BatchNormalization()(x) # testing out batch normalisation
x = layers.MaxPooling2D((2,2))(x)
x = layers.Conv2D(32,(3,3),activation = 'relu')(x)
x = layers.BatchNormalization()(x)
x = layers.MaxPooling2D((2,2))(x)
x = layers.Conv2D(32,(3,3,),activation = 'relu')(x)
x = layers.BatchNormalization()(x)
x = layers.MaxPooling2D((2,2))(x)
x = layers.Conv2D(32,(3,3,),activation = 'relu')(x)
x = layers.BatchNormalization()(x)
x = layers.MaxPooling2D((2,2))(x)
x = layers.Flatten()(x)

# classification model
x = layers.Dense(128,activation = 'relu')(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(256,activation = 'relu')(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(256,activation = 'relu')(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(128,activation = 'relu')(x)

output = layers.Dense(2)(x) # output layer
```
```python
# assemble the model
model3 = tf.keras.Model(
    inputs = i,
    outputs = output
)
```
```python
# model summary
model3.summary()
```
```
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_2 (InputLayer)        [(None, 160, 160, 3)]     0

 model (Functional)          (None, 160, 160, 3)       0

 sequential_2 (Sequential)   (None, 160, 160, 3)       0

 conv2d (Conv2D)             (None, 158, 158, 64)      1792

 batch_normalization (BatchN  (None, 158, 158, 64)     256
 ormalization)

 max_pooling2d (MaxPooling2D  (None, 79, 79, 64)       0
 )

 conv2d_1 (Conv2D)           (None, 77, 77, 32)        18464

 batch_normalization_1 (Batc  (None, 77, 77, 32)       128
 hNormalization)

 max_pooling2d_1 (MaxPooling  (None, 38, 38, 32)       0
 2D)

 conv2d_2 (Conv2D)           (None, 36, 36, 32)        9248

 batch_normalization_2 (Batc  (None, 36, 36, 32)       128
 hNormalization)

 max_pooling2d_2 (MaxPooling  (None, 18, 18, 32)       0
 2D)

 conv2d_3 (Conv2D)           (None, 16, 16, 32)        9248

 batch_normalization_3 (Batc  (None, 16, 16, 32)       128
 hNormalization)

 max_pooling2d_3 (MaxPooling  (None, 8, 8, 32)         0
 2D)

 flatten (Flatten)           (None, 2048)              0

 dense (Dense)               (None, 128)               262272

 dropout (Dropout)           (None, 128)               0

 dense_1 (Dense)             (None, 256)               33024

 dropout_1 (Dropout)         (None, 256)               0

 dense_2 (Dense)             (None, 256)               65792

 dropout_2 (Dropout)         (None, 256)               0

 dense_3 (Dense)             (None, 128)               32896

 dense_4 (Dense)             (None, 2)                 258

=================================================================
Total params: 433,634
Trainable params: 433,314
Non-trainable params: 320
_________________________________________________________________
```
Compiling and training our model
```python
model3.compile(optimizer='adam',
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy'])
history3 = model3.fit(train_dataset,epochs = 20,
           validation_data = validation_dataset)
```
```
Epoch 1/20
63/63 [==============================] - 19s 126ms/step - loss: 0.7263 - accuracy: 0.5485 - val_loss: 0.7751 - val_accuracy: 0.4950
Epoch 2/20
63/63 [==============================] - 7s 110ms/step - loss: 0.6589 - accuracy: 0.6175 - val_loss: 0.8419 - val_accuracy: 0.5037
Epoch 3/20
63/63 [==============================] - 7s 111ms/step - loss: 0.6376 - accuracy: 0.6400 - val_loss: 1.3663 - val_accuracy: 0.5025
Epoch 4/20
63/63 [==============================] - 7s 112ms/step - loss: 0.6268 - accuracy: 0.6565 - val_loss: 1.0169 - val_accuracy: 0.5025
Epoch 5/20
63/63 [==============================] - 7s 110ms/step - loss: 0.6043 - accuracy: 0.6800 - val_loss: 1.0871 - val_accuracy: 0.5087
Epoch 6/20
63/63 [==============================] - 7s 111ms/step - loss: 0.5923 - accuracy: 0.6815 - val_loss: 0.7161 - val_accuracy: 0.5718
Epoch 7/20
63/63 [==============================] - 7s 111ms/step - loss: 0.5794 - accuracy: 0.6865 - val_loss: 0.8859 - val_accuracy: 0.5941
Epoch 8/20
63/63 [==============================] - 7s 112ms/step - loss: 0.5647 - accuracy: 0.7115 - val_loss: 0.5950 - val_accuracy: 0.7017
Epoch 9/20
63/63 [==============================] - 7s 112ms/step - loss: 0.5643 - accuracy: 0.7110 - val_loss: 0.6149 - val_accuracy: 0.6473
Epoch 10/20
63/63 [==============================] - 7s 112ms/step - loss: 0.5491 - accuracy: 0.7220 - val_loss: 0.5725 - val_accuracy: 0.7153
Epoch 11/20
63/63 [==============================] - 7s 111ms/step - loss: 0.5263 - accuracy: 0.7340 - val_loss: 0.5334 - val_accuracy: 0.7116
Epoch 12/20
63/63 [==============================] - 7s 110ms/step - loss: 0.5173 - accuracy: 0.7430 - val_loss: 0.5797 - val_accuracy: 0.7079
Epoch 13/20
63/63 [==============================] - 7s 113ms/step - loss: 0.5204 - accuracy: 0.7595 - val_loss: 0.6090 - val_accuracy: 0.6448
Epoch 14/20
63/63 [==============================] - 7s 110ms/step - loss: 0.5201 - accuracy: 0.7440 - val_loss: 0.5564 - val_accuracy: 0.7129
Epoch 15/20
63/63 [==============================] - 7s 110ms/step - loss: 0.5040 - accuracy: 0.7495 - val_loss: 0.5220 - val_accuracy: 0.7463
Epoch 16/20
63/63 [==============================] - 7s 110ms/step - loss: 0.4914 - accuracy: 0.7475 - val_loss: 0.4976 - val_accuracy: 0.7611
Epoch 17/20
63/63 [==============================] - 7s 111ms/step - loss: 0.4916 - accuracy: 0.7575 - val_loss: 0.5879 - val_accuracy: 0.6646
Epoch 18/20
63/63 [==============================] - 7s 112ms/step - loss: 0.4768 - accuracy: 0.7715 - val_loss: 0.5342 - val_accuracy: 0.7401
Epoch 19/20
63/63 [==============================] - 7s 110ms/step - loss: 0.4621 - accuracy: 0.7875 - val_loss: 0.5113 - val_accuracy: 0.7500
Epoch 20/20
63/63 [==============================] - 7s 110ms/step - loss: 0.4557 - accuracy: 0.7825 - val_loss: 0.5560 - val_accuracy: 0.7611
```

Plot of model training and validation performance
```python
# plot of model3 performance in training and validation sets
plot_history(model3)
```
![_config.yml]({{ site.baseurl }}/images/mod3-perf-hw3.png)

The validation accuracy of the model stabilises **between 70% - 76%** after the 10th epoch. Model3's performance is significantly better than model1's performance, which ranged between 60% - 70%. Model3 is also less prone to overfitting. After the 7th epoch, our training and validation accuracy also remains quite stable. Thus, model3 exhibits less overfitting than model1 and model2.

## 5. Transfer Learning
In this section, we're going to use a pre-trained image classification model that someone else has already built to see if we can get even better performance. More specifically, we'll be incorporating the MobileNetV2 model into our existing model to see if we can achieve a higher classification accuracy than 70% - 76%.

```python
IMG_SHAPE = IMG_SIZE + (3,) # set input size

# load transfer learning model into notebook
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

# create transfer learning model layer
i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```
```python
# creating model 4
i = tf.keras.Input(shape=(160, 160, 3))
# preprocessing
x = preprocessor(i)
# data augmentation
x = data_augmentation(x)
# transfer learning model
x = base_model_layer(x)
x = tf.keras.layers.GlobalMaxPooling2D()(x)
x = tf.keras.layers.Dropout(0.2)(x)
# output layer
output = layers.Dense(2)(x)

# assemble model 4
model4 = tf.keras.Model(
    inputs = i,
    outputs = output
)
```

Now let's fit and train our model and see how transfer learning improves our ability to classify cats and dogs.
```python
model4.compile(optimizer='adam',
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy'])
history4 = model4.fit(train_dataset,epochs = 20,validation_data = validation_dataset)
```
```
Epoch 1/20
63/63 [==============================] - 90s 112ms/step - loss: 0.3062 - accuracy: 0.9200 - val_loss: 0.0854 - val_accuracy: 0.9666
Epoch 2/20
63/63 [==============================] - 6s 89ms/step - loss: 0.2873 - accuracy: 0.9305 - val_loss: 0.0829 - val_accuracy: 0.9728
Epoch 3/20
63/63 [==============================] - 6s 91ms/step - loss: 0.2954 - accuracy: 0.9350 - val_loss: 0.0654 - val_accuracy: 0.9802
Epoch 4/20
63/63 [==============================] - 6s 89ms/step - loss: 0.2193 - accuracy: 0.9400 - val_loss: 0.1046 - val_accuracy: 0.9715
Epoch 5/20
63/63 [==============================] - 6s 90ms/step - loss: 0.2419 - accuracy: 0.9425 - val_loss: 0.0988 - val_accuracy: 0.9715
Epoch 6/20
63/63 [==============================] - 6s 89ms/step - loss: 0.2575 - accuracy: 0.9370 - val_loss: 0.0681 - val_accuracy: 0.9752
Epoch 7/20
63/63 [==============================] - 6s 89ms/step - loss: 0.2088 - accuracy: 0.9420 - val_loss: 0.0606 - val_accuracy: 0.9827
Epoch 8/20
63/63 [==============================] - 6s 89ms/step - loss: 0.1863 - accuracy: 0.9495 - val_loss: 0.0602 - val_accuracy: 0.9851
Epoch 9/20
63/63 [==============================] - 6s 91ms/step - loss: 0.2082 - accuracy: 0.9405 - val_loss: 0.0673 - val_accuracy: 0.9814
Epoch 10/20
63/63 [==============================] - 6s 90ms/step - loss: 0.1554 - accuracy: 0.9500 - val_loss: 0.0662 - val_accuracy: 0.9814
Epoch 11/20
63/63 [==============================] - 6s 91ms/step - loss: 0.2252 - accuracy: 0.9400 - val_loss: 0.0731 - val_accuracy: 0.9790
Epoch 12/20
63/63 [==============================] - 6s 90ms/step - loss: 0.1730 - accuracy: 0.9480 - val_loss: 0.0544 - val_accuracy: 0.9889
Epoch 13/20
63/63 [==============================] - 6s 90ms/step - loss: 0.1805 - accuracy: 0.9440 - val_loss: 0.0583 - val_accuracy: 0.9814
Epoch 14/20
63/63 [==============================] - 6s 90ms/step - loss: 0.1492 - accuracy: 0.9570 - val_loss: 0.0627 - val_accuracy: 0.9790
Epoch 15/20
63/63 [==============================] - 6s 89ms/step - loss: 0.1608 - accuracy: 0.9510 - val_loss: 0.0553 - val_accuracy: 0.9851
Epoch 16/20
63/63 [==============================] - 6s 90ms/step - loss: 0.1835 - accuracy: 0.9460 - val_loss: 0.0408 - val_accuracy: 0.9851
Epoch 17/20
63/63 [==============================] - 6s 90ms/step - loss: 0.1928 - accuracy: 0.9510 - val_loss: 0.0491 - val_accuracy: 0.9876
Epoch 18/20
63/63 [==============================] - 6s 90ms/step - loss: 0.1706 - accuracy: 0.9520 - val_loss: 0.0473 - val_accuracy: 0.9864
Epoch 19/20
63/63 [==============================] - 6s 90ms/step - loss: 0.1719 - accuracy: 0.9560 - val_loss: 0.0442 - val_accuracy: 0.9889
Epoch 20/20
63/63 [==============================] - 6s 89ms/step - loss: 0.1693 - accuracy: 0.9520 - val_loss: 0.0413 - val_accuracy: 0.9839
```
```python
# plot of model4 performance in training and validation sets
plot_history(model4)
```
![_config.yml]({{ site.baseurl }}/images/mod4-perf-hw3.png)

The validation accuracy of the model stabilised **between 97% - 99%** while training. This transfer learning model's performance is better than model3 by roughly 25%. This model also exhibits no overfitting. Training performance is always worse than validation performance.

## 6. Score on Test Data
The transfer learning model has the highest validation accuracy, so we will apply it to our test data and see how it performs on unseen information.

```python
model4.evaluate(test_dataset)
```
```
6/6 [==============================] - 2s 116ms/step - loss: 0.0546 - accuracy: 0.9792
[0.054569970816373825, 0.9791666865348816]
```
This model achieves an accuracy of roughly 98% on our test data. It's really good.

## 7. Conclusion
Transfer learning models are very powerful and should be used when possible. However, if they are unavailable, we still can rest easy knowing that we have the ability to build decently good image classification models via tools such as preprocessing, data augmentation, and convolutional neural networks.
