---
layout: post
title:  "Using NLP to Identify Fake News"
categories: blog assignment
permalink: posts/HW-4
author: Shannan Liu
---

## Introduction
In this post, we will create a fake news classifier using Tensorflow.

## 1. Acquiring Data
### Data Source
The data for we're using comes from the following article:

Ahmed H, Traore I, Saad S. (2017) â€œDetection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).

### Imports
```python
# imports
import pandas as pd
import numpy as np
import re
import string
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow import keras
import matplotlib as mpl

mpl.rcParams['figure.dpi'] = 120
mpl.style.use('seaborn')

# text vectorization layer
from tensorflow.keras.layers import TextVectorization

# natural language toolkit
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords') # download corpus of stopwords
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
```

### Getting Data
```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
train = pd.read_csv(train_url,index_col = 0)
train.head()
```
![_config.yml]({{ site.baseurl }}/images/train-hw4.png)

In our training data frame, each row corresponds to an article:
- the `title` column contains the titles of each article
- the `text` column contains the full article post
- the `fake` column is 0 if the article is true  and 1 if the article contains misinformation

## 2. Make Dataset

Before we build our model to classify this data, we're going to remove *stopwords* from our `title` and `text` columns. *Stopwords* are words that are considered uninformative for our classification task. For instance, words such as "the", "and", "but" do not contain much meaning.

Then, we'll convert our dataframe into a `tf.keras.Dataset` object that has 2 inputs and 1 output. Our 2 inputs will be `text` and `title`, and our single output will consist of the fake column.

```python
# get a set of stopwords from nltk corpus
stop_words = set(stopwords.words("english"))
# example of removing stop words from our data
# and using a word tokenizer
ex_sent = train['title'].iloc[0] # example sentence
filtered_sent = []

# for every word in the word-tokenized sentence
# if it is not a stop word, add it to our filtered sentence list
for w in word_tokenize(ex_sent):
  if w not in stop_words:
    filtered_sent.append(w)

# print results to show difference
print(word_tokenize(ex_sent))
print(filtered_sent)
```

```
['Merkel', ':', 'Strong', 'result', 'for', 'Austria', "'s", 'FPO', "'big", 'challenge', "'", 'for', 'other', 'parties']
['Merkel', ':', 'Strong', 'result', 'Austria', "'s", 'FPO', "'big", 'challenge', "'", 'parties']
```

```python
def make_dataset(df):
  """
  Remove stopwords from our data,
  and create a Tensorflow Dataset from our
  Pandas dataframe
  """
  df = df.copy()

  # light preprocessing for our string columns
  for col in ['title','text']:
    # remove stopwords
    df[col] = df[col].apply(lambda x: ' '.join([w for w in word_tokenize(x) if w not in stop_words]))
  # put our information into a dataset
  data = tf.data.Dataset.from_tensor_slices(
    (# dictionary for input data/features
      {"title":df[['title']],
       "text":df[['text']]},

     # dictionary for output data/labels
     {"fake":df[['fake']]}
    ))

  # batching our dataset
  # will cause training to occur on chunks of data
  # this may reduce training accuracy, but it will
  # make training faster
  data = data.batch(100)
  return data
data = make_dataset(train)
```

### Train-Validation Split
Now we're going to split this data into training and validation sets.

```python
#80% train, 20% val
train_size = int(0.8*len(data))
val_size = int(0.2*len(data))

train = data.take(train_size) # data[:train_size]
val = data.skip(train_size) # data[train_size:]

len(train),len(val)
```
```
(180, 45)
```

### Base Rate
The base rate refers to the accuracy of a model that always makes the same guess. We can determine the base rate for this data set by examining the labels on the training set.

```python
labels_iterator = train.unbatch().map(lambda x, label: label).as_numpy_iterator()
num_true = int(sum([x['fake']==0 for x in list(labels_iterator)]))
print(f"Number of truthful articles = {num_true}")
labels_iterator = train.unbatch().map(lambda x, label: label).as_numpy_iterator()
num_fake = int(sum([x['fake'] for x in list(labels_iterator)]))
print(f"Number of articles containing fake news = {num_fake}")
```
```
Number of truthful articles = 8603
Number of articles containing fake news = 9397
```
```python
print(f"Base rate = {round(num_fake/(num_true + num_fake),4)*100}%")
```
```
Base rate = 52.21%
```

Our base rate will be 52.21%. In other words, if our model were to guess that all articles are fake news, it would achieve this accuracy.

### TextVectorization

```python
# preparing a text vectorization layer for tf model
title_vocab_size = 2000
text_vocab_size = 4000

def standardization(input_data):
  """
  This function will remove punctuation from our text data
  and make each word lower-cased
  """
  lowercase = tf.strings.lower(input_data)
  no_punctuation = tf.strings.regex_replace(lowercase,
                                '[%s]' % re.escape(string.punctuation),'')
  return no_punctuation

# this will turn titles into numbers based on their frequency ranking
title_vectorize_layer = TextVectorization(
    # this performs the same function as the standardization function above
    standardize=standardization,
    max_tokens=title_vocab_size, # only consider this many words
    output_mode='int', # assign integer value to each word based on frequency ranking of words in the training set
    output_sequence_length=500) # turn every title into a vector of 500 numbers

# this will turn texts into numbers based on their frequency ranking
text_vectorize_layer = TextVectorization(
    # this performs the same function as the standardization function above
    standardize=standardization,
    max_tokens=text_vocab_size, # only consider this many words
    output_mode='int', # assign integer value to each word based on frequency ranking of words in the training set
    output_sequence_length=1000) # turn every title into a vector of 500 numbers
```

### Vectorize Titles and Texts
```python
# vectorize news titles
title_vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
# vectorize text titles
text_vectorize_layer.adapt(train.map(lambda x, y: x['text']))
```

## 3. Create Models

In this section, we will create 3 models to answer the following question
> When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?

Our first model will only use article titles to make predictions. The second model will utilise article texts to make predictions, and the third model will use both the title and full text of articles to make predictions.

```python
# first, define our inputs for functional model
title_input = tf.keras.Input(
  shape = (1,),
  name = 'title', # MUST make name of input same as dictionary key name of input in dataset
  dtype = 'string' # what input it expects to see
)

text_input = tf.keras.Input(
  shape = (1,),
  name = 'text', # MUST make name of input same as dictionary key name of input in dataset
  dtype = 'string' # what input it expects to see
)
```
### Model 1: using article titles to make predictions
```python
title_features = title_vectorize_layer(title_input) # apply this "function TextVectorization layer" to lyrics_input

# output_dim = 0/1 = true/fake
title_features = layers.Embedding(input_dim=title_vocab_size, output_dim = 10, name="title_embedding")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(512, activation='relu')(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(256, activation='relu')(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(128, activation='relu')(title_features)
output = layers.Dense(2, name="fake")(title_features)

model1 = tf.keras.Model(
    inputs = title_input,
    outputs = output
)

keras.utils.plot_model(model1)
```
![_config.yml]({{ site.baseurl }}/images/title-mod-hw4.png)

```python
model1.compile(optimizer='adam',
               loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
               metrics = ['accuracy'])
```
```python
early_stopping = tf.keras.callbacks.EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=10, # how many epochs to wait before stopping
    restore_best_weights=True,
)
def fit_mod(model):
  history = model.fit(train,
                      validation_data=val,
                      epochs = 20,
                      callbacks=[early_stopping])
  return history

history1 = fit_mod(model1)
```
```
Epoch 1/20
/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning:

Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.

180/180 [==============================] - 7s 32ms/step - loss: 0.6889 - accuracy: 0.5400 - val_loss: 0.6132 - val_accuracy: 0.6033
Epoch 2/20
180/180 [==============================] - 5s 30ms/step - loss: 0.1764 - accuracy: 0.9304 - val_loss: 0.0544 - val_accuracy: 0.9834
Epoch 3/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0629 - accuracy: 0.9788 - val_loss: 0.0392 - val_accuracy: 0.9867
Epoch 4/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0552 - accuracy: 0.9813 - val_loss: 0.0522 - val_accuracy: 0.9856
Epoch 5/20
180/180 [==============================] - 5s 29ms/step - loss: 0.0488 - accuracy: 0.9830 - val_loss: 0.0291 - val_accuracy: 0.9894
Epoch 6/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0448 - accuracy: 0.9858 - val_loss: 0.0275 - val_accuracy: 0.9910
Epoch 7/20
180/180 [==============================] - 5s 28ms/step - loss: 0.0430 - accuracy: 0.9859 - val_loss: 0.0265 - val_accuracy: 0.9915
Epoch 8/20
180/180 [==============================] - 6s 31ms/step - loss: 0.0435 - accuracy: 0.9866 - val_loss: 0.0409 - val_accuracy: 0.9858
Epoch 9/20
180/180 [==============================] - 8s 43ms/step - loss: 0.0378 - accuracy: 0.9881 - val_loss: 0.0446 - val_accuracy: 0.9847
Epoch 10/20
180/180 [==============================] - 7s 38ms/step - loss: 0.0392 - accuracy: 0.9882 - val_loss: 0.0279 - val_accuracy: 0.9899
Epoch 11/20
180/180 [==============================] - 6s 36ms/step - loss: 0.0384 - accuracy: 0.9877 - val_loss: 0.0268 - val_accuracy: 0.9901
Epoch 12/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0306 - accuracy: 0.9907 - val_loss: 0.0563 - val_accuracy: 0.9804
Epoch 13/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0300 - accuracy: 0.9913 - val_loss: 0.0317 - val_accuracy: 0.9890
Epoch 14/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0265 - accuracy: 0.9924 - val_loss: 0.0450 - val_accuracy: 0.9836
Epoch 15/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0258 - accuracy: 0.9925 - val_loss: 0.0295 - val_accuracy: 0.9890
Epoch 16/20
180/180 [==============================] - 4s 22ms/step - loss: 0.0262 - accuracy: 0.9919 - val_loss: 0.1002 - val_accuracy: 0.9618
```
```python
from matplotlib import pyplot as plt
def plot_history(history):
  plt.plot(history.history["accuracy"], label = "training")
  plt.plot(history.history["val_accuracy"], label = "validation")
  plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
  plt.title("Training vs Validation Accuracy")
  plt.legend()
  plt.show()

plot_history(history1)
```
![_config.yml]({{ site.baseurl }}/images/title-mod-perf-hw4.png)

The **validation accuracy stabilises between 96% - 99%**. The model performs quite well considering it only uses title information to discern whether an article is fake news or not!

### Model 2: using article text to make predictions

```python
text_features = text_vectorize_layer(text_input) # apply this "function TextVectorization layer" to lyrics_input

# output_dim = 0/1 = true/fake
text_features = layers.Embedding(input_dim=text_vocab_size, output_dim = 10, name="text_embedding")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(512, activation='relu')(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(256, activation='relu')(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(128, activation='relu')(text_features)
output = layers.Dense(2, name="fake")(text_features)

model2 = tf.keras.Model(
    inputs = text_input,
    outputs = output
)

keras.utils.plot_model(model2)

model2.compile(optimizer='adam',
               loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
               metrics = ['accuracy'])

keras.utils.plot_model(model2)
```
![_config.yml]({{ site.baseurl }}/images/text-mod-hw4.png)

```python
history2 = fit_mod(model2)
```
```
Epoch 1/20
/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning:

Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.

180/180 [==============================] - 9s 43ms/step - loss: 0.5094 - accuracy: 0.6932 - val_loss: 0.1603 - val_accuracy: 0.9416
Epoch 2/20
180/180 [==============================] - 7s 37ms/step - loss: 0.1169 - accuracy: 0.9598 - val_loss: 0.0796 - val_accuracy: 0.9751
Epoch 3/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0700 - accuracy: 0.9774 - val_loss: 0.0606 - val_accuracy: 0.9825
Epoch 4/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0485 - accuracy: 0.9852 - val_loss: 0.0590 - val_accuracy: 0.9852
Epoch 5/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0332 - accuracy: 0.9902 - val_loss: 0.0533 - val_accuracy: 0.9856
Epoch 6/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0303 - accuracy: 0.9900 - val_loss: 0.0548 - val_accuracy: 0.9856
Epoch 7/20
180/180 [==============================] - 7s 38ms/step - loss: 0.0333 - accuracy: 0.9878 - val_loss: 0.0503 - val_accuracy: 0.9863
Epoch 8/20
180/180 [==============================] - 7s 41ms/step - loss: 0.0281 - accuracy: 0.9909 - val_loss: 0.0561 - val_accuracy: 0.9847
Epoch 9/20
180/180 [==============================] - 7s 38ms/step - loss: 0.0168 - accuracy: 0.9949 - val_loss: 0.0559 - val_accuracy: 0.9870
Epoch 10/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0186 - accuracy: 0.9942 - val_loss: 0.1133 - val_accuracy: 0.9652
Epoch 11/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0230 - accuracy: 0.9927 - val_loss: 0.0607 - val_accuracy: 0.9847
Epoch 12/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0122 - accuracy: 0.9967 - val_loss: 0.0583 - val_accuracy: 0.9874
Epoch 13/20
180/180 [==============================] - 8s 43ms/step - loss: 0.0094 - accuracy: 0.9974 - val_loss: 0.0650 - val_accuracy: 0.9858
Epoch 14/20
180/180 [==============================] - 7s 38ms/step - loss: 0.0116 - accuracy: 0.9963 - val_loss: 0.0730 - val_accuracy: 0.9845
Epoch 15/20
180/180 [==============================] - 7s 38ms/step - loss: 0.0113 - accuracy: 0.9963 - val_loss: 0.0671 - val_accuracy: 0.9858
Epoch 16/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0148 - accuracy: 0.9942 - val_loss: 0.0662 - val_accuracy: 0.9854
Epoch 17/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.1055 - val_accuracy: 0.9771
```

```python
plot_history(history2)
```
![_config.yml]({{ site.baseurl }}/images/text-mod-perf-hw4.png)

The **validation accuracy stabilises between 97% - 98%**. Model performance is comparable to our title-only model.

### Model 3: using article title and text to make predictions
```python
main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(512, activation='relu')(main)
main = layers.Dense(256, activation='relu')(main)
main = layers.Dense(128, activation='relu')(main)
output = layers.Dense(2, name="fake")(main)

model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)

keras.utils.plot_model(model3)
```
![_config.yml]({{ site.baseurl }}/images/all-mod-hw4.png)

```python
model3.compile(optimizer='adam',
               loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
               metrics = ['accuracy'])
history3 = fit_mod(model3)
```
```
Epoch 1/20
180/180 [==============================] - 15s 72ms/step - loss: 0.0179 - accuracy: 0.9928 - val_loss: 0.0089 - val_accuracy: 0.9973
Epoch 2/20
180/180 [==============================] - 17s 92ms/step - loss: 0.0090 - accuracy: 0.9966 - val_loss: 0.0292 - val_accuracy: 0.9944
Epoch 3/20
180/180 [==============================] - 15s 84ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0122 - val_accuracy: 0.9971
Epoch 4/20
180/180 [==============================] - 17s 96ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.0073 - val_accuracy: 0.9978
Epoch 5/20
180/180 [==============================] - 16s 87ms/step - loss: 0.0082 - accuracy: 0.9963 - val_loss: 0.0082 - val_accuracy: 0.9971
Epoch 6/20
180/180 [==============================] - 16s 87ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0087 - val_accuracy: 0.9964
Epoch 7/20
180/180 [==============================] - 16s 91ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0109 - val_accuracy: 0.9984
Epoch 8/20
180/180 [==============================] - 15s 86ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.0140 - val_accuracy: 0.9955
Epoch 9/20
180/180 [==============================] - 15s 83ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0058 - val_accuracy: 0.9984
Epoch 10/20
180/180 [==============================] - 14s 78ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0085 - val_accuracy: 0.9982
Epoch 11/20
180/180 [==============================] - 12s 68ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0458 - val_accuracy: 0.9901
Epoch 12/20
180/180 [==============================] - 15s 84ms/step - loss: 0.0085 - accuracy: 0.9966 - val_loss: 0.0119 - val_accuracy: 0.9978
Epoch 13/20
180/180 [==============================] - 13s 74ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0081 - val_accuracy: 0.9980
Epoch 14/20
180/180 [==============================] - 16s 91ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0091 - val_accuracy: 0.9980
Epoch 15/20
180/180 [==============================] - 16s 91ms/step - loss: 2.8781e-05 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 0.9982
Epoch 16/20
180/180 [==============================] - 13s 75ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.0183 - val_accuracy: 0.9980
Epoch 17/20
180/180 [==============================] - 13s 71ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0084 - val_accuracy: 0.9982
Epoch 18/20
180/180 [==============================] - 12s 67ms/step - loss: 7.4885e-04 - accuracy: 0.9998 - val_loss: 0.0118 - val_accuracy: 0.9984
Epoch 19/20
180/180 [==============================] - 15s 83ms/step - loss: 5.8074e-04 - accuracy: 0.9998 - val_loss: 0.0165 - val_accuracy: 0.9978
```
```python
plot_history(history3)
```
![_config.yml]({{ site.baseurl }}/images/all-mod-perf-hw4.png)

The **validation accuracy stabilises at around 99.8%**. This model's performance is better than both of our previous models.

## 4. Model Evaluation

From the training and validation we've performed above, we can see that model3, which uses both the title and text data, performs the best. To ensure that it is the best model, we can now test all of our models on unseen data and see which one achieves the highest accuracy.

### Getting Test Data
```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test = pd.read_csv(test_url,index_col = 0)
test.shape # shape of the data
```
```
(22449, 3)
```
```python
test = make_dataset(test) # convert test dataframe into a tf dataset
```
### Scoring the Models
```python
loss1, acc1 = model1.evaluate(test)
```
```
225/225 [==============================] - 2s 8ms/step - loss: 0.1749 - accuracy: 0.9533
```
```python
loss2, acc2 = model2.evaluate(test)
```
```
225/225 [==============================] - 3s 14ms/step - loss: 0.1292 - accuracy: 0.9650
```
```python
loss3, acc3 = model3.evaluate(test)
```
```
225/225 [==============================] - 4s 17ms/step - loss: 0.0151 - accuracy: 0.9969
```

- Model1 (article title only) has a test accuracy of 95.33%
- Model2 (article text only) has a test accuracy of 96.50%
- Model3 (article title and text) has a test accuracy of 99.69%

As expected, the model that uses both title and text information performs the best (affirming what we observed during the training and validation phase).

## 5. Embedding Visualization
Let's visualise the embeddings that our best model (model3) learned. These embeddings will show us word patterns/associations that our model found most useful when distinguishing real news from fake news. We will examineboth 2-dimensional and 3-dimensional embeddings, and we will discuss a few words whose locations are interpretable.

We will create our plots via a function called `embedding_plot()`, which is written below:
```python
from sklearn.decomposition import PCA
import plotly.express as px

def embedding_plot(model, embedding_layer_name, vectorize_layer, title, r3 = True, size = 0.5, **kwargs):
  """
  Create a word embedding visualisation
  If r3, then plot a 3D word embedding visualisation
  Otherwise, plot a 2D word embedding visualisation

  Parameters
  model: the NLP model with relevant embedding layer
  embedding_layer_name: name of the embedding layer in the model
  vectorize_layer: the text_vectorization layer in the mode
  title: plot title
  r3: whether or not to plot a 3D or 2D plot
  size: size of plotted points
  """
  # get the weights from the embedding layer
  weights = model.get_layer(embedding_layer_name).get_weights()[0]

  # get the vocabulary from our relevant vectorizing layer
  vocab = vectorize_layer.get_vocabulary()

  if r3:
    # plot in R3
    pca = PCA(n_components = 3,)
    weights_pca = pca.fit_transform(weights)
    embedding_df = pd.DataFrame({
        'word' : vocab,
        'x0'   : weights[:,0],
        'x1'   : weights[:,1],
        'x2'   : weights[:,2]
    })

    # create figure
    fig = px.scatter_3d(embedding_df,
                        x = "x0",
                        y = "x1",
                        z = "x2",
                        size = [size]*len(embedding_df),
                        hover_name = "word",
                        title = title,
                        **kwargs)
  else:
    # plot in R2
    pca = PCA(n_components = 2)
    weights_pca = pca.fit_transform(weights)

    embedding_df = pd.DataFrame({
        'word' : vocab,
        'x0'   : weights[:,0],
        'x1'   : weights[:,1]
    })

    # create figure
    fig = px.scatter(embedding_df,
                    x = "x0",
                    y = "x1",
                    size = [size]*len(embedding_df),
                    hover_name = "word",
                    title = title,
                     **kwargs)

  fig.update_traces(marker=dict(line=dict(width=0)),
                    selector=dict(mode='markers'))
  return fig
```

### Title Vectorization Embedding Plots
First, we create the embedding plots for our `title_embedding` layer.
#### 3D Embedding Plot
```python
fig = embedding_plot(model = model3,
                     embedding_layer_name = 'title_embedding',
                     vectorize_layer=title_vectorize_layer,
                     title='3D Title Vectorization Embedding Plot',
                     r3 = True,
                     size = 0.5,
                     opacity = 0.5)
fig.show()
```
{% include 3D-title-embedding.html %}

#### 2D Embedding Plot
```python
fig = embedding_plot(model = model3,
                     embedding_layer_name = 'title_embedding',
                     vectorize_layer=title_vectorize_layer,
                     title='2D Title Vectorization Embedding Plot',
                     r3 = False,
                     size = 0.1,
                     opacity = 0.5)
fig.show()
```
{% include 2D-title-embedding.html %}

These plots show that political diction, politically charged words, and attention grabbing language such as "breaking", "exposes", and "caught" are good indicators of fake news.

For instance:
1. GOP
2. Hillary (Clinton)
3. Socialist
4. DOJ
5. ISIS
6. Liberal
7. Exposes
8. Caught
9. Breaking (news)
10. Terrorists
11. Racist

### Text Vectorization Embedding Plots
Now let's examine the embedding plots for our `text_embedding` layer
#### 3D Embedding Plot
```python
fig = embedding_plot(model = model3,
                     embedding_layer_name = 'text_embedding',
                     vectorize_layer=text_vectorize_layer,
                     title='3D Text Vectorization Embedding Plot',
                     r3 = True,
                     size = 0.5,
                     opacity = 0.5)
fig.show()
```
{% include 3D-text-embedding.html %}

#### 2D Embedding Plot
```python
fig = embedding_plot(model = model3,
                     embedding_layer_name = 'text_embedding',
                     vectorize_layer=text_vectorize_layer,
                     title='2D Text Vectorization Embedding Plot',
                     r3 = False,
                     size = 0.1,
                     opacity = 0.5)
fig.show()
fig.write_html("2D text embedding.html")
```
{% include 2D-text-embedding.html %}

We can observe a similar pattern in text embedding plots. Words related to politics or tabloidy language such as "reportedly" are good indicators of fake news.

For instance:
1. GOP
2. Daily Mail
3. Apparently
4. Narrative
5. Reportedly
6. Shocking
7. (Tucker) Carlson
8. Fox (News)
