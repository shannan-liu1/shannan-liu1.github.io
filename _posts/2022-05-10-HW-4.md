---
layout: post
title:  "Using NLP to Identify Fake News"
categories: blog assignment
permalink: posts/HW-4
author: Shannan Liu
---

## Introduction
In this post, we will create a fake news classifier using Tensorflow.

## 1. Acquiring Data
### Data Source
The data for we're using comes from the following article:

Ahmed H, Traore I, Saad S. (2017) â€œDetection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).

### Imports
```python
# imports
import pandas as pd
import numpy as np
import re
import string
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow import keras
import matplotlib as mpl

mpl.rcParams['figure.dpi'] = 120
mpl.style.use('seaborn')

# text vectorization layer
from tensorflow.keras.layers import TextVectorization

# natural language toolkit
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords') # download corpus of stopwords
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
```

### Getting Data
```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
train = pd.read_csv(train_url,index_col = 0)
train.head()
```
![_config.yml]({{ site.baseurl }}/images/train-hw4.png)

In our training data frame, each row corresponds to an article:
- the `title` column contains the titles of each article
- the `text` column contains the full article post
- the `fake` column is 0 if the article is true  and 1 if the article contains misinformation

## 2. Make Dataset

Before we build our model to classify this data, we're going to remove *stopwords* from our `title` and `text` columns. *Stopwords* are words that are considered uninformative for our classification task. For instance, words such as "the", "and", "but" do not contain much meaning.

Then, we'll convert our dataframe into a `tf.keras.Dataset` object that has 2 inputs and 1 output. Our 2 inputs will be `text` and `title`, and our single output will consist of the fake column.

```python
# get a set of stopwords from nltk corpus
stop_words = set(stopwords.words("english"))
# example of removing stop words from our data
# and using a word tokenizer
ex_sent = train['title'].iloc[0] # example sentence
filtered_sent = []

# for every word in the word-tokenized sentence
# if it is not a stop word, add it to our filtered sentence list
for w in word_tokenize(ex_sent):
  if w not in stop_words:
    filtered_sent.append(w)

# print results to show difference
print(word_tokenize(ex_sent))
print(filtered_sent)
```

```
['Merkel', ':', 'Strong', 'result', 'for', 'Austria', "'s", 'FPO', "'big", 'challenge', "'", 'for', 'other', 'parties']
['Merkel', ':', 'Strong', 'result', 'Austria', "'s", 'FPO', "'big", 'challenge', "'", 'parties']
```

```python
def make_dataset(df):
  """
  Remove stopwords from our data,
  and create a Tensorflow Dataset from our
  Pandas dataframe
  """
  df = df.copy()

  # light preprocessing for our string columns
  for col in ['title','text']:
    # remove stopwords
    df[col] = df[col].apply(lambda x: ' '.join([w for w in word_tokenize(x) if w not in stop_words]))
  # put our information into a dataset
  data = tf.data.Dataset.from_tensor_slices(
    (# dictionary for input data/features
      {"title":df[['title']],
       "text":df[['text']]},

     # dictionary for output data/labels
     {"fake":df[['fake']]}
    ))

  # batching our dataset
  # will cause training to occur on chunks of data
  # this may reduce training accuracy, but it will
  # make training faster
  data = data.batch(100)
  return data
data = make_dataset(train)
```

### Train-Validation Split
Now we're going to split this data into training and validation sets.

```python
#80% train, 20% val
train_size = int(0.8*len(data))
val_size = int(0.2*len(data))

train = data.take(train_size) # data[:train_size]
val = data.skip(train_size) # data[train_size:]

len(train),len(val)
```
```
(180, 45)
```

### Base Rate
The base rate refers to the accuracy of a model that always makes the same guess. We can determine the base rate for this data set by examining the labels on the training set.

```python
labels_iterator = train.unbatch().map(lambda x, label: label).as_numpy_iterator()
num_true = int(sum([x['fake']==0 for x in list(labels_iterator)]))
print(f"Number of truthful articles = {num_true}")
labels_iterator = train.unbatch().map(lambda x, label: label).as_numpy_iterator()
num_fake = int(sum([x['fake'] for x in list(labels_iterator)]))
print(f"Number of articles containing fake news = {num_fake}")
```
```
Number of truthful articles = 8603
Number of articles containing fake news = 9397
```
```python
print(f"Base rate = {round(num_fake/(num_true + num_fake),4)*100}%")
```
```
Base rate = 52.21%
```

Our base rate will be $52.21\%$. In other words, if our model were to guess that all articles are fake news, it would achieve this accuracy.

### TextVectorization

```python
#preparing a text vectorization layer for tf model
title_vocab_size = 2000
text_vocab_size = 4000

def standardization(input_data):
  """
  This function will remove punctuation from our text data
  and make each word lower-cased
  """
  lowercase = tf.strings.lower(input_data)
  no_punctuation = tf.strings.regex_replace(lowercase,
                                '[%s]' % re.escape(string.punctuation),'')
  return no_punctuation

# this will turn titles into numbers based on their frequency ranking
title_vectorize_layer = TextVectorization(
    # this performs the same function as the standardization function above
    standardize=standardization,
    max_tokens=title_vocab_size, # only consider this many words
    output_mode='int', # assign integer value to each word based on frequency ranking of words in the training set
    output_sequence_length=500) # turn every title into a vector of 500 numbers

# this will turn texts into numbers based on their frequency ranking
text_vectorize_layer = TextVectorization(
    # this performs the same function as the standardization function above
    standardize=standardization,
    max_tokens=text_vocab_size, # only consider this many words
    output_mode='int', # assign integer value to each word based on frequency ranking of words in the training set
    output_sequence_length=1000) # turn every title into a vector of 500 numbers
```

### Vectorize Titles and Texts
```python
# vectorize news titles
title_vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
# vectorize text titles
text_vectorize_layer.adapt(train.map(lambda x, y: x['text']))
```

## 3. Create Models

In this section, we will create 3 models to answer the following question
> When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?

Our first model will only use article titles to make predictions. The second model will utilise article texts to make predictions, and the third model will use both the title and full text of articles to make predictions.

```python
# first, define our inputs for functional model
title_input = tf.keras.Input(
  shape = (1,),
  name = 'title', # MUST make name of input same as dictionary key name of input in dataset
  dtype = 'string' # what input it expects to see
)

text_input = tf.keras.Input(
  shape = (1,),
  name = 'text', # MUST make name of input same as dictionary key name of input in dataset
  dtype = 'string' # what input it expects to see
)
```
### Model 1: using article titles to make predictions
```python
title_features = title_vectorize_layer(title_input) # apply this "function TextVectorization layer" to lyrics_input

# output_dim = 0/1 = true/fake
title_features = layers.Embedding(input_dim=title_vocab_size, output_dim = 10, name="title_embedding")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(512, activation='relu')(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(256, activation='relu')(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(128, activation='relu')(title_features)
output = layers.Dense(2, name="fake")(title_features)

model1 = tf.keras.Model(
    inputs = title_input,
    outputs = output
)

keras.utils.plot_model(model1)
```
![_config.yml]({{ site.baseurl }}/images/mod-title-hw4.png)

```python
model1.compile(optimizer='adam',
               loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
               metrics = ['accuracy'])
```
```python
early_stopping = tf.keras.callbacks.EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=10, # how many epochs to wait before stopping
    restore_best_weights=True,
)
def fit_mod(model):
  history = model.fit(train,
                      validation_data=val,
                      epochs = 20,
                      callbacks=[early_stopping])
  return history

history1 = fit_mod(model1)
```
```
Epoch 1/20
/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning:

Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.

180/180 [==============================] - 7s 32ms/step - loss: 0.6889 - accuracy: 0.5400 - val_loss: 0.6132 - val_accuracy: 0.6033
Epoch 2/20
180/180 [==============================] - 5s 30ms/step - loss: 0.1764 - accuracy: 0.9304 - val_loss: 0.0544 - val_accuracy: 0.9834
Epoch 3/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0629 - accuracy: 0.9788 - val_loss: 0.0392 - val_accuracy: 0.9867
Epoch 4/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0552 - accuracy: 0.9813 - val_loss: 0.0522 - val_accuracy: 0.9856
Epoch 5/20
180/180 [==============================] - 5s 29ms/step - loss: 0.0488 - accuracy: 0.9830 - val_loss: 0.0291 - val_accuracy: 0.9894
Epoch 6/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0448 - accuracy: 0.9858 - val_loss: 0.0275 - val_accuracy: 0.9910
Epoch 7/20
180/180 [==============================] - 5s 28ms/step - loss: 0.0430 - accuracy: 0.9859 - val_loss: 0.0265 - val_accuracy: 0.9915
Epoch 8/20
180/180 [==============================] - 6s 31ms/step - loss: 0.0435 - accuracy: 0.9866 - val_loss: 0.0409 - val_accuracy: 0.9858
Epoch 9/20
180/180 [==============================] - 8s 43ms/step - loss: 0.0378 - accuracy: 0.9881 - val_loss: 0.0446 - val_accuracy: 0.9847
Epoch 10/20
180/180 [==============================] - 7s 38ms/step - loss: 0.0392 - accuracy: 0.9882 - val_loss: 0.0279 - val_accuracy: 0.9899
Epoch 11/20
180/180 [==============================] - 6s 36ms/step - loss: 0.0384 - accuracy: 0.9877 - val_loss: 0.0268 - val_accuracy: 0.9901
Epoch 12/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0306 - accuracy: 0.9907 - val_loss: 0.0563 - val_accuracy: 0.9804
Epoch 13/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0300 - accuracy: 0.9913 - val_loss: 0.0317 - val_accuracy: 0.9890
Epoch 14/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0265 - accuracy: 0.9924 - val_loss: 0.0450 - val_accuracy: 0.9836
Epoch 15/20
180/180 [==============================] - 4s 21ms/step - loss: 0.0258 - accuracy: 0.9925 - val_loss: 0.0295 - val_accuracy: 0.9890
Epoch 16/20
180/180 [==============================] - 4s 22ms/step - loss: 0.0262 - accuracy: 0.9919 - val_loss: 0.1002 - val_accuracy: 0.9618
```
```python
from matplotlib import pyplot as plt
def plot_history(history):
  plt.plot(history.history["accuracy"], label = "training")
  plt.plot(history.history["val_accuracy"], label = "validation")
  plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
  plt.title("Training vs Validation Accuracy")
  plt.legend()
  plt.show()

plot_history(history1)
```
![_config.yml]({{ site.baseurl }}/images/title-mod-perf-hw4.png)


```python
history2
```

```
Epoch 1/20
/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)
180/180 [==============================] - 8s 38ms/step - loss: 0.4707 - accuracy: 0.7188 - val_loss: 0.1653 - val_accuracy: 0.9272
Epoch 2/20
180/180 [==============================] - 7s 37ms/step - loss: 0.1065 - accuracy: 0.9630 - val_loss: 0.1285 - val_accuracy: 0.9438
Epoch 3/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0666 - accuracy: 0.9763 - val_loss: 0.0581 - val_accuracy: 0.9829
Epoch 4/20
180/180 [==============================] - 7s 38ms/step - loss: 0.0412 - accuracy: 0.9879 - val_loss: 0.0535 - val_accuracy: 0.9858
Epoch 5/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0327 - accuracy: 0.9898 - val_loss: 0.0696 - val_accuracy: 0.9843
Epoch 6/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0333 - accuracy: 0.9888 - val_loss: 0.0560 - val_accuracy: 0.9874
Epoch 7/20
180/180 [==============================] - 8s 42ms/step - loss: 0.0250 - accuracy: 0.9918 - val_loss: 0.0514 - val_accuracy: 0.9874
Epoch 8/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0219 - accuracy: 0.9926 - val_loss: 0.0495 - val_accuracy: 0.9892
Epoch 9/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0244 - accuracy: 0.9925 - val_loss: 0.0486 - val_accuracy: 0.9874
Epoch 10/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0220 - accuracy: 0.9932 - val_loss: 0.0582 - val_accuracy: 0.9825
Epoch 11/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0138 - accuracy: 0.9960 - val_loss: 0.0632 - val_accuracy: 0.9856
Epoch 12/20
180/180 [==============================] - 7s 38ms/step - loss: 0.0094 - accuracy: 0.9978 - val_loss: 0.1017 - val_accuracy: 0.9762
Epoch 13/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0856 - val_accuracy: 0.9789
Epoch 14/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 0.0758 - val_accuracy: 0.9865
Epoch 15/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0191 - accuracy: 0.9934 - val_loss: 0.0810 - val_accuracy: 0.9804
Epoch 16/20
180/180 [==============================] - 7s 37ms/step - loss: 0.0145 - accuracy: 0.9951 - val_loss: 0.0721 - val_accuracy: 0.9858
Epoch 17/20
180/180 [==============================] - 7s 38ms/step - loss: 0.0084 - accuracy: 0.9969 - val_loss: 0.1116 - val_accuracy: 0.9697
Epoch 18/20
180/180 [==============================] - 7s 38ms/step - loss: 0.0172 - accuracy: 0.9940 - val_loss: 0.0765 - val_accuracy: 0.9845
```


```python
history3
```

```
Epoch 1/20
180/180 [==============================] - 40s 59ms/step - loss: 0.0189 - accuracy: 0.9937 - val_loss: 0.0187 - val_accuracy: 0.9946
Epoch 2/20
180/180 [==============================] - 10s 57ms/step - loss: 0.0066 - accuracy: 0.9974 - val_loss: 0.0236 - val_accuracy: 0.9953
Epoch 3/20
180/180 [==============================] - 10s 57ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.0093 - val_accuracy: 0.9982
Epoch 4/20
180/180 [==============================] - 10s 58ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.0081 - val_accuracy: 0.9973
Epoch 5/20
180/180 [==============================] - 10s 56ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0068 - val_accuracy: 0.9987
Epoch 6/20
180/180 [==============================] - 10s 57ms/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.0066 - val_accuracy: 0.9982
Epoch 7/20
180/180 [==============================] - 10s 57ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0077 - val_accuracy: 0.9984
Epoch 8/20
180/180 [==============================] - 10s 56ms/step - loss: 8.8323e-05 - accuracy: 1.0000 - val_loss: 0.0341 - val_accuracy: 0.9964
Epoch 9/20
180/180 [==============================] - 10s 56ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0116 - val_accuracy: 0.9973
Epoch 10/20
180/180 [==============================] - 10s 56ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.0076 - val_accuracy: 0.9982
Epoch 11/20
180/180 [==============================] - 10s 57ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0133 - val_accuracy: 0.9975
Epoch 12/20
180/180 [==============================] - 10s 57ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.0111 - val_accuracy: 0.9975
Epoch 13/20
180/180 [==============================] - 10s 56ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.0174 - val_accuracy: 0.9957
Epoch 14/20
180/180 [==============================] - 10s 56ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.1287 - val_accuracy: 0.9755
Epoch 15/20
180/180 [==============================] - 10s 55ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0130 - val_accuracy: 0.9973
```

## Model eval

```
225/225 [==============================] - 2s 8ms/step - loss: 0.1749 - accuracy: 0.9533

225/225 [==============================] - 3s 14ms/step - loss: 0.1292 - accuracy: 0.9650

225/225 [==============================] - 4s 17ms/step - loss: 0.0151 - accuracy: 0.9969
```
